{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import keras\n",
        "avg_sens = [0,0,0,0]\n",
        "avg_spec = [0,0,0,0]\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "#from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout,Maximum\n",
        "#from keras.layers.core import Lambda, RepeatVector, Reshape\n",
        "#from keras.layers.convolutional import Conv2D, Conv2DTranspose,Conv3D,Conv3DTranspose\n",
        "#from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D,MaxPooling3D\n",
        "#from keras.layers.merge import concatenate, add\n",
        "#from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "#from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "#from skimage.io import imread, imshow, concatenate_images\n",
        "#from skimage.transform import resize\n",
        "\n",
        "def standardize(image):\n",
        "\n",
        "  standardized_image = np.zeros(image.shape)\n",
        "\n",
        "  #\n",
        "  \n",
        "      # iterate over the `z` dimension\n",
        "  for z in range(image.shape[2]):\n",
        "      # get a slice of the image \n",
        "      # at channel c and z-th dimension `z`\n",
        "      image_slice = image[:,:,z]\n",
        "\n",
        "      # subtract the mean from image_slice\n",
        "      centered = image_slice - np.mean(image_slice)\n",
        "      \n",
        "      # divide by the standard deviation (only if it is different from zero)\n",
        "      if(np.std(centered)!=0):\n",
        "          centered = centered/np.std(centered) \n",
        "\n",
        "      # update  the slice of standardized image\n",
        "      # with the scaled centered and scaled image\n",
        "      standardized_image[:, :, z] = centered\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return standardized_image\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred, epsilon=0.00001):\n",
        "    \"\"\"\n",
        "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
        "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
        "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
        "    \n",
        "    \"\"\"\n",
        "    axis = (0,1,2)\n",
        "    dice_numerator = 2. * K.sum(y_true * y_pred, axis=axis) + epsilon\n",
        "    dice_denominator = K.sum(y_true*y_true, axis=axis) + K.sum(y_pred*y_pred, axis=axis) + epsilon\n",
        "    return K.mean((dice_numerator)/(dice_denominator))\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)\n",
        "\n",
        "def compute_class_sens_spec(pred, label, class_num):\n",
        "    \"\"\"\n",
        "    Compute sensitivity and specificity for a particular example\n",
        "    for a given class.\n",
        "    Args:\n",
        "        pred (np.array): binary arrary of predictions, shape is\n",
        "                         (num classes, height, width, depth).\n",
        "        label (np.array): binary array of labels, shape is\n",
        "                          (num classes, height, width, depth).\n",
        "        class_num (int): number between 0 - (num_classes -1) which says\n",
        "                         which prediction class to compute statistics\n",
        "                         for.\n",
        "    Returns:\n",
        "        sensitivity (float): precision for given class_num.\n",
        "        specificity (float): recall for given class_num\n",
        "    \"\"\"\n",
        "\n",
        "    # extract sub-array for specified class\n",
        "    class_pred = pred[:,:,:,class_num]\n",
        "    class_label = label[:,:,:,class_num]\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # compute true positives, false positives, \n",
        "    # true negatives, false negatives\n",
        "    tp = np.sum((class_pred == 1) & (class_label == 1))\n",
        "    tn = np.sum((class_pred == 0) & (class_label == 0))\n",
        "    fp = np.sum((class_pred == 1) & (class_label == 0))\n",
        "    fn = np.sum((class_pred == 0) & (class_label == 1))\n",
        "\n",
        "    # compute sensitivity and specificity\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return sensitivity, specificity\n",
        "\n",
        "\n",
        "def get_sens_spec_df(pred, label):\n",
        "    patch_metrics = pd.DataFrame(\n",
        "        columns = ['No Tumor Region',\n",
        "                    'Whole Tumor'], \n",
        "        index = ['Sensitivity',\n",
        "                 'Specificity'])\n",
        "    \n",
        "    for i, class_name in enumerate(patch_metrics.columns):\n",
        "        sens, spec = compute_class_sens_spec(pred, label, i)\n",
        "        avg_sens[i] += sens\n",
        "        avg_spec[i] += spec\n",
        "        patch_metrics.loc['Sensitivity', class_name] = round(sens,4)\n",
        "        patch_metrics.loc['Specificity', class_name] = round(spec,4)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "model_axis1 = load_model('/content/drive/MyDrive/models/2dclasses2_model_axis1.h5',custom_objects = {'dice_coef_loss' : dice_coef_loss , 'dice_coef' : dice_coef})\n",
        "model_axis2 = load_model('/content/drive/MyDrive/models/2dclasses2_model_axis2.h5',custom_objects = {'dice_coef_loss' : dice_coef_loss , 'dice_coef' : dice_coef})\n",
        "model_axis3 = load_model('/content/drive/MyDrive/models/2dclasses2_model_axis3.h5',custom_objects = {'dice_coef_loss' : dice_coef_loss , 'dice_coef' : dice_coef})\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/BRATS2018TRAIN/HGG'\n",
        "all_images = os.listdir(path)\n",
        "#print(len(all_images))\n",
        "all_images.sort()\n",
        "data = np.zeros((240,240,155,4))\n",
        "image_data2=np.zeros((240,240,155))\n",
        "\n",
        "\n",
        "import nibabel as nib\n",
        "mean_loss = 0\n",
        "mean_accu = 0\n",
        "for image_num in range(180,210):\n",
        "    data = np.zeros((240,240,155,4))\n",
        "    #print(epochs)\n",
        "    print(\"Entering Image\" , image_num)\n",
        "\n",
        "# data preprocessing starts here\n",
        "\n",
        "    x = all_images[image_num]\n",
        "    #print(x)\n",
        "    folder_path = path + '/' + x;\n",
        "    modalities = os.listdir(folder_path)\n",
        "    modalities.sort()\n",
        "    #data = []\n",
        "    w = 0\n",
        "    for j in range(len(modalities)):\n",
        "      image_path = folder_path + '/' + modalities[j]\n",
        "      if not(image_path.find('seg.nii') == -1):\n",
        "        img = nib.load(image_path);\n",
        "        image_data2 = img.get_data()\n",
        "        image_data2 = np.asarray(image_data2)\n",
        "        #print(\"Entered ground truth\")\n",
        "      else:\n",
        "        img = nib.load(image_path);\n",
        "        image_data = img.get_data()\n",
        "        image_data = np.asarray(image_data)\n",
        "        image_data = standardize(image_data)\n",
        "        data[:,:,:,w] = image_data\n",
        "        #print(\"Entered modality\")\n",
        "        w = w+1\n",
        "        \n",
        "    image_data2[image_data2 == 4] = 1\n",
        "    image_data2[image_data2 == 2] = 1\n",
        "    image_data2 = tf.keras.utils.to_categorical(image_data2, num_classes = 2)\n",
        "\n",
        "    data_axis1=data\n",
        "    data_axis2=np.moveaxis(data,1,0)\n",
        "    data_axis3=np.moveaxis(data,2,0)\n",
        "\n",
        "\n",
        "    Y_hat_axis1 = model_axis1.predict(x=data_axis1)\n",
        "    Y_hat_axis2 = model_axis2.predict(x=data_axis2)\n",
        "    Y_hat_axis3 = model_axis3.predict(x=data_axis3)\n",
        "        \n",
        "    Y_hat_axis2 = np.moveaxis(Y_hat_axis2,0,1)\n",
        "    Y_hat_axis3 = np.moveaxis(Y_hat_axis3,0,2)\n",
        "\n",
        "    Y_hat_average = (Y_hat_axis1+Y_hat_axis2+Y_hat_axis3)/3\n",
        "    dice_acc=K.eval(dice_coef(Y_hat_average,image_data2))\n",
        "    dice_loss=1- dice_acc\n",
        "#     print(dice_acc)\n",
        "\n",
        "    Y_hat_average_onehot = np.argmax(Y_hat_average,axis = -1)\n",
        "    Y_hat_average_onehot = tf.keras.utils.to_categorical(Y_hat_average_onehot,num_classes=2)\n",
        "\n",
        "    get_sens_spec_df(Y_hat_average_onehot,image_data2)\n",
        "\n",
        "    mean_loss += dice_loss\n",
        "    mean_accu += dice_acc\n",
        "    \n",
        "print()\n",
        "print(\"Mean Dice Loss\" , mean_loss/30)\n",
        "print(\"Mean Dice Coefficient(Accuracy)\" , mean_accu/30)\n",
        "print()\n",
        "print(\"Mean Sensitivity for class 0\" , avg_sens[0]/30)\n",
        "print(\"Mean Specificity for class 0\" , avg_spec[0]/30)\n",
        "print()\n",
        "print(\"Mean Sensitivity for class 1\" , avg_sens[1]/30)\n",
        "print(\"Mean Specificity for class 1\" , avg_spec[1]/30)\n",
        "print()\n",
        "print(\"Mean Sensitivity for class 2\" , avg_sens[2]/30)\n",
        "print(\"Mean Specificity for class 2\" , avg_spec[2]/30)\n",
        "print()\n",
        "print(\"Mean Sensitivity for class 3\" , avg_sens[3]/30)\n",
        "print(\"Mean Specificity for class 3\" , avg_spec[3]/30)\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import nibabel as nib\n",
        "x = all_images[193]\n",
        "print(\"Results on image number 182\")\n",
        "folder_path = path + '/' + x;\n",
        "modalities = os.listdir(folder_path)\n",
        "modalities.sort()\n",
        "data = np.zeros((240,240,155,4))\n",
        "#data = []\n",
        "w = 0\n",
        "for j in range(len(modalities)):\n",
        "  #print(modalities[j])\n",
        "\n",
        "  image_path = folder_path + '/' + modalities[j]\n",
        "  if not(image_path.find('seg.nii') == -1):\n",
        "    img = nib.load(image_path);\n",
        "    image_data2 = img.get_data()\n",
        "    image_data2 = np.asarray(image_data2)\n",
        "    print(\"Entered ground truth\")\n",
        "  else:\n",
        "    img = nib.load(image_path);\n",
        "    image_data = img.get_data()\n",
        "    image_data = np.asarray(image_data)\n",
        "    image_data = standardize(image_data)\n",
        "    data[:,:,:,w] = image_data\n",
        "    print(\"Entered modality\")\n",
        "    w = w+1\n",
        "    \n",
        "    \n",
        "image_data2[image_data2 == 4] = 1\n",
        "image_data2[image_data2 == 2] = 1\n",
        "\n",
        "data_axis1=data\n",
        "data_axis2=np.moveaxis(data,1,0)\n",
        "data_axis3=np.moveaxis(data,2,0)\n",
        "\n",
        "image_data2_axis1=image_data2\n",
        "image_data2_axis2=np.moveaxis(image_data2,1,0)\n",
        "image_data2_axis3=np.moveaxis(image_data2,2,0)\n",
        "\n",
        "Y_hat_axis1 = model_axis1.predict(x=data_axis1)\n",
        "Y_hat_axis2 = model_axis2.predict(x=data_axis2)\n",
        "Y_hat_axis3 = model_axis3.predict(x=data_axis3)\n",
        "\n",
        "Y_hat_axis2 = np.moveaxis(Y_hat_axis2,0,1)\n",
        "Y_hat_axis3 = np.moveaxis(Y_hat_axis3,0,2)\n",
        "\n",
        "Y_hat_average = (Y_hat_axis1+Y_hat_axis2+Y_hat_axis3)/3\n",
        "\n",
        "Y_hat_average_onehot = np.argmax(Y_hat_average,axis = -1)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#import matplotlib.pyplot as plt\n",
        "img = data[:,:,75,0]\n",
        "imgplot = plt.imshow(img)\n",
        "plt.title('Flair Modality 75th slice alon')\n",
        "plt.savefig('Flair')\n",
        "plt.show()\n",
        "\n",
        "img2 = image_data2[:,:,75]\n",
        "imgplot2 = plt.imshow(img2)\n",
        "plt.title('Ground Truth of 75th slice')\n",
        "plt.savefig('Ground_Truth')\n",
        "plt.show()\n",
        "\n",
        "img3 = Y_hat_average_onehot[:,:,75]\n",
        "imgplot3 = plt.imshow(img3)\n",
        "plt.title('Our Segmentation -> 75th slice')\n",
        "plt.savefig('Our Segmentation')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#img4 = img + img3\n",
        "#imgplot4 = plt.imshow(img4)\n",
        "#plt.title('Our Segmentation -> 75th slice')\n",
        "#plt.savefig('Our Segmentation masked')\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sys \n",
        "for lim in range(2):\n",
        "  for j in range(155):\n",
        "    z =data[:,:,j,0]\n",
        "    y=data[:,j,:,0]\n",
        "    x=data[j,:,:,0]\n",
        "    img = np.concatenate((x, y, z), axis=1)\n",
        "    imgplot = plt.imshow(img)\n",
        "    #plt.savefig('Flair')\n",
        "    plt.show()\n",
        "    #plt.close()\n",
        "  for m in range(155,240):\n",
        "    z =data[:,:,154,0]\n",
        "    y=data[:,m,:,0]\n",
        "    x=data[m,:,:,0]\n",
        "    img = np.concatenate((x, y, z), axis=1)\n",
        "    imgplot = plt.imshow(img)\n",
        "    #plt.savefig('Flair')\n",
        "    plt.show()\n",
        "   #plt.close()\n",
        "    lim=1\n",
        "    j=0\n",
        "    if m==240:\n",
        "      m=0\n"
      ],
      "metadata": {
        "id": "aY7Eqgu1lS86"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}